{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spotify_million_playlist_dataset in Datenbank schreiben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init des skripts\n",
    "\n",
    "imports und wichtige variablen deffinieren\n",
    "\n",
    " - unter dem Pfad PATH liegen die json Dateien des spotify_million_playlist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from getpass import getpass\n",
    "from io import StringIO\n",
    "import psycopg2\n",
    "\n",
    "PATH = \"D:/SpotifyDataset/spotify_million_playlist_dataset/data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktion: readFile2dictLists(filename)\n",
    "\n",
    "Iteration über playlists und tracks in einem mitgegebenen File\n",
    "\n",
    "Speichen alle playlists, tracks, albums, interprets und playlist_enthaelt_song als Dictionaries in den 5 Listen nach ER-Modell\n",
    "\n",
    "Die 5 *_dictList Listen sind Listen, die mit Dictionaries gefüllt werden, da so die Erstellung von DataSets effizienter läuft, als wenn wir direkt mit Dataframes von Pandas arbeiten und jedes Element nacheinander in ein Dataframe einfügen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile2dictLists(filename):\n",
    "    with open(PATH + \"/\" + filename) as f:\n",
    "        #Convert to string and replace\n",
    "        obj_str = json.dumps(json.load(f)).replace(';', ',')\n",
    "\n",
    "        #Get obj back with replacement\n",
    "        data = json.loads(obj_str)\n",
    "\n",
    "        playlists = data[\"playlists\"]\n",
    "        for playlist in playlists:\n",
    "            \n",
    "            dPlaylist = {\"Playlist_ID\": playlist[\"pid\"], \"name\": playlist[\"name\"].replace(\"\\\\\", \"\\\\\\\\\"), \"collaborative\": playlist[\"collaborative\"], \"modified_at\": playlist[\"modified_at\"], \"num_tracks\": playlist[\"num_tracks\"], \"num_albums\": playlist[\"num_albums\"], \"num_followers\": playlist[\"num_followers\"], \"num_edits\": playlist[\"num_edits\"], \"duration_ms\": playlist[\"duration_ms\"], \"num_artists\": playlist[\"num_artists\"]}\n",
    "            playlists_dictList.append(dPlaylist)\n",
    "\n",
    "            for track in playlist[\"tracks\"]:\n",
    "                playlist_enthaelt_song.append({\"enthaelt_ID\": None, \"Playlist\": playlist[\"pid\"], \"Song\": track[\"track_uri\"], \"pos\": track[\"pos\"]})\n",
    "                \n",
    "                dTrack = {\"Song_ID\": track[\"track_uri\"], \"Interpret\": track[\"artist_uri\"], \"Album\": track[\"album_uri\"], \"track_name\": track[\"track_name\"].replace(\"\\\\\", \"\\\\\\\\\") , \"duration_ms\": track[\"duration_ms\"]}\n",
    "                songs_dictList.append(dTrack)\n",
    "\n",
    "                dInterpret = {\"Interpret_ID\": track[\"artist_uri\"], \"artist_name\": track[\"artist_name\"].replace(\"\\\\\", \"\\\\\\\\\") }\n",
    "                artists_dictList.append(dInterpret)\n",
    "\n",
    "                dAlbum = {\"Album_ID\": track[\"album_uri\"], \"album_name\": track[\"album_name\"].replace(\"\\\\\", \"\\\\\\\\\") }\n",
    "                albums_dictList.append(dAlbum)\n",
    "        \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funktion: listsToFrames()\n",
    "\n",
    "## Dictionaries zu Dataframes umwandeln\n",
    "\n",
    "Jetzt können wir schön schnell die Listen von Dicts in Dataframes umwandeln und dabei Duplicate entfernen.\n",
    "dfPlaylist_enthaelt_song wird niemals Duplikate aufweisen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listsToFrames():\n",
    "    dfAlbums =  pd.DataFrame(albums_dictList).drop_duplicates(ignore_index=True)\n",
    "    dfArtists = pd.DataFrame(artists_dictList).drop_duplicates(ignore_index=True)\n",
    "    dfSongs =   pd.DataFrame(songs_dictList).drop_duplicates(ignore_index=True)\n",
    "    dfPlaylists =   pd.DataFrame(playlists_dictList).drop_duplicates(ignore_index=True)\n",
    "\n",
    "    dfPlaylist_enthaelt_song = pd.DataFrame(playlist_enthaelt_song)\n",
    "    dfPlaylist_enthaelt_song['enthaelt_ID'] = range(0, len(dfPlaylist_enthaelt_song))\n",
    "    dfPlaylist_enthaelt_song[\"Playlist\"] = dfPlaylist_enthaelt_song[\"Playlist\"].astype(int)\n",
    "    dfPlaylist_enthaelt_song[\"pos\"] = dfPlaylist_enthaelt_song[\"pos\"].astype(int)\n",
    "\n",
    "    return [dfAlbums, dfArtists, dfSongs, dfPlaylists, dfPlaylist_enthaelt_song]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alle Dataframes nacheinander anzeigen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenbank Kram\n",
    "Jetzt haben wir die Dataframes fertig, jetzt müssen wir sie in die Datenbank bekommen, also kommt im Folgenden Zeug zum Thema Datenbank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login abfrage für Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = input(\"Bitte User für DB Eingeben:\")\n",
    "pswd = getpass(\"Bitte Password für DB Eingeben:\")\n",
    "\n",
    "login_param = f\"dbname='spotify_test' user='{user}' host='localhost' port='5432' password='{pswd}'\"\n",
    "#login_param = f\"dbname='orent001_spotify_test' user='{user}' host='localhost' port='9001' password='{pswd}'\"\n",
    "# #login_param = f\"dbname='orent001_spotify' user='{user}' host='localhost' port='9001' password='{pswd}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktion zur Verbindungsherstellung mit der Datenbnak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the PostgreSQL database...\n",
      "('UTF8',)\n",
      "Connection successful\n"
     ]
    }
   ],
   "source": [
    "def connect(params_dic):\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # connect to the PostgreSQL server\n",
    "        print('Connecting to the PostgreSQL database...')\n",
    "        conn = psycopg2.connect(params_dic)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"SHOW client_encoding;\")\n",
    "        print(cur.fetchone())\n",
    "        cur.close()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        sys.exit(1) \n",
    "    print(\"Connection successful\")\n",
    "    return conn\n",
    "\n",
    "conn = connect(login_param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes in Datenbank einfügen\n",
    "\n",
    "Statt hier viel auszuprobieren wie bisher, haben wir hierfür und direkt nach einem performance Benchmark umgesehen, wie wir am effizientesten mit psycog2 Daten in die postgresDB laden können.\n",
    "\n",
    "### Quelle:\n",
    "https://naysan.ca/2020/05/09/pandas-to-postgresql-using-psycopg2-bulk-insert-performance-benchmark/\n",
    "\n",
    "Das beste Ergebnis ist, copy_from zu verwenden und aus einem gespeicherten File zu laden. Wir haben für das \"speichern\" oder besser gesagt, buffern StringIO verwendet.\n",
    "\n",
    "Es folgt die Methode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_from_stringio(conn, df, table):\n",
    "    \"\"\"\n",
    "    Here we are going save the dataframe in memory \n",
    "    and use copy_from() to copy it to the table\n",
    "    \"\"\"\n",
    "    # save dataframe to an in memory buffer\n",
    "    buffer = StringIO()\n",
    "    df.to_csv(buffer, index=False, header=False, sep=\";\", encoding=\"utf-8\")\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.copy_from(buffer, table, sep=\";\")\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cursor.close()\n",
    "        sys.exit()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabellen erstellen\n",
    "Wir haben jetzt alles wichtige für das einfügen von DataFrames in eine Tabelle in der Datenbank. Was wir noch nicht haben, sind die Tabellen in der Datenbank. Die wollen wir auch Skriptbasiert anlegen. Aber falls es sie schon gibt, sollten die alten zu erst gelöscht werden.\n",
    "Wir brauchen folgende Tabellen:\n",
    " - Album\n",
    " - Interpret\n",
    " - Song\n",
    " - Playlist\n",
    " - P_enthaelt_S\n",
    "\n",
    "analog dazu stecken wir auch die Dataframes in der selben Reihenfolge in eine Liste, um in einer Iteration später copy_from_stringio() mit den richtigen Para,etern aufrufen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsAlbum = [\"Album_ID\", \"album_name\"]\n",
    "tsArtist = [\"Interpret_ID\", \"artist_name\"]\n",
    "tsSong = [\"Song_ID\", \"Interpret\", \"Album\", \"track_name\", \"duration_ms\"]\n",
    "tsPlaylist = [\"Playlist_ID \", \"name\", \"collaborative\", \"modified_at\", \"num_tracks\", \"num_albums\", \"num_followers\", \"num_edits\", \"duration_ms\", \"num_artists\"]\n",
    "tsPlaylist_enthaelt_song = [\"enthaelt_ID\", \"Playlist\", \"Song\", \"pos\"]\n",
    "\n",
    "tablenames = [\"album\", \"interpret\", \"song\", \"playlist\", \"p_enthaelt_s\"]\n",
    "temp_tablenames = [\"temp_album\", \"temp_interpret\", \"temp_song\", \"temp_playlist\", \"temp_p_enthaelt_s\"]\n",
    "tableshemas = [tsAlbum, tsArtist, tsSong, tsPlaylist, tsPlaylist_enthaelt_song]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktion zum Löschen der Tabellen\n",
    "\n",
    "Überlegung: tut's nicht auch das selbe, wenn wir alles löschen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteTable(tablename):\n",
    "    #Creating a cursor object using the cursor() method\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    #Doping table if already exists\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {tablename}\")\n",
    "\n",
    "    #Commit your changes in the database\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "for tablename in temp_tablenames[::-1]:\n",
    "    deleteTable(tablename)\n",
    "for tablename in tablenames[::-1]:\n",
    "    deleteTable(tablename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querries zum Erstellen der Tabellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTables(prod_tablenames, ref_tablenames):\n",
    "    albumQuerry = f\"\"\"\n",
    "    CREATE TABLE {prod_tablenames[0]} (\n",
    "    {tableshemas[0][0]} varchar(255) PRIMARY KEY,\n",
    "    {tableshemas[0][1]} varchar(510)\n",
    "    )\"\"\"\n",
    "\n",
    "    interpretQuerry = f\"\"\"\n",
    "    CREATE TABLE {prod_tablenames[1]} (\n",
    "    {tableshemas[1][0]} varchar(255) PRIMARY KEY,\n",
    "    {tableshemas[1][1]} varchar(510)\n",
    "    )\"\"\"\n",
    "\n",
    "    songQuerry = f\"\"\"\n",
    "    CREATE TABLE {prod_tablenames[2]} (\n",
    "    {tableshemas[2][0]} varchar(255) PRIMARY KEY,\n",
    "    {tableshemas[2][1]} varchar(255) REFERENCES {ref_tablenames[1]} ({tableshemas[1][0]}),\n",
    "    {tableshemas[2][2]} varchar(255) REFERENCES {ref_tablenames[0]} ({tableshemas[0][0]}),\n",
    "    {tableshemas[2][3]} varchar(510),\n",
    "    {tableshemas[2][4]} int\n",
    "    )\"\"\"\n",
    "\n",
    "    playlistQuerry = f\"\"\"\n",
    "    CREATE TABLE {prod_tablenames[3]} (\n",
    "    {tableshemas[3][0]} varchar(255) PRIMARY KEY,\n",
    "    {tableshemas[3][1]} varchar(510),\n",
    "    {tableshemas[3][2]} boolean,\n",
    "    {tableshemas[3][3]} int,\n",
    "    {tableshemas[3][4]} int,\n",
    "    {tableshemas[3][5]} int,\n",
    "    {tableshemas[3][6]} int,\n",
    "    {tableshemas[3][7]} int,\n",
    "    {tableshemas[3][8]} int,\n",
    "    {tableshemas[3][9]} int\n",
    "    )\"\"\"\n",
    "\n",
    "    p_enthaelt_sQuerry = f\"\"\"\n",
    "    CREATE TABLE {prod_tablenames[4]} (\n",
    "    {tableshemas[4][0]} int PRIMARY KEY,\n",
    "    {tableshemas[4][1]} varchar(255) REFERENCES {ref_tablenames[3]} ({tableshemas[3][0]}),\n",
    "    {tableshemas[4][2]} varchar(255) REFERENCES {ref_tablenames[2]} ({tableshemas[2][0]}),\n",
    "    {tableshemas[4][3]} int\n",
    "\n",
    "    )\"\"\"\n",
    "\n",
    "    commands = (albumQuerry, interpretQuerry, songQuerry, playlistQuerry, p_enthaelt_sQuerry)\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    # create table one by one\n",
    "    for command in commands:\n",
    "        cur.execute(command)\n",
    "    # close communication with the PostgreSQL database server\n",
    "    cur.close()\n",
    "    # commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "createTables(tablenames,tablenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframesInDatenbankSchreiben():\n",
    "    # temporäre Tabellen erstellen:\n",
    "    createTables(temp_tablenames, tablenames)\n",
    "\n",
    "    for i in range(0,5):\n",
    "        # dataframe in temp tabelle kopieren:\n",
    "        copy_from_stringio(conn, dataframes[i], temp_tablenames[i])\n",
    "\n",
    "        # von temp in prod tabelle kopieren:\n",
    "        verschiebQuerry = f\"INSERT INTO {tablenames[i]} SELECT * FROM {temp_tablenames[i]} ON CONFLICT DO NOTHING\"\n",
    "        if (i == 4):\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"SELECT COUNT(enthaelt_id) FROM p_enthaelt_s;\")\n",
    "            countRows = cur.fetchone()[0]\n",
    "            cur.close()\n",
    "            verschiebQuerry = f\"INSERT INTO {tablenames[i]} (enthaelt_ID, Playlist, Song, pos) SELECT enthaelt_ID + {countRows}, Playlist, Song, pos FROM {temp_tablenames[i]} ON CONFLICT DO NOTHING\"\n",
    "        cur = conn.cursor()\n",
    "        try:\n",
    "            cur.execute(verschiebQuerry)\n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            print(\"Error: %s\" % error)\n",
    "            print(verschiebQuerry)\n",
    "            conn.rollback()\n",
    "            cur.close()\n",
    "            sys.exit()\n",
    "        else:\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "\n",
    "    # temporäre Tabellen löschen:\n",
    "    for ele in temp_tablenames[::-1]:\n",
    "        deleteTable(ele)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIER PASSIERT DAS WICHTIGE ZEUG!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:23<00:00, 16.69s/it]\n"
     ]
    }
   ],
   "source": [
    "allFilenames = os.listdir(PATH)[:5]\n",
    "for dieNaechsten10filenamen in tqdm(np.array_split(allFilenames, 5)):\n",
    "\n",
    "    # Listen leeren:\n",
    "    artists_dictList = []\n",
    "    albums_dictList = []\n",
    "    songs_dictList = []\n",
    "    playlists_dictList = []\n",
    "    playlist_enthaelt_song = []\n",
    "    \n",
    "    for filename in dieNaechsten10filenamen:\n",
    "\n",
    "        # aktuelles File in Liste laden\n",
    "        readFile2dictLists(filename)\n",
    "    \n",
    "    # die Listen aus den 10 Files zu Dataframes machen \n",
    "    dataframes = listsToFrames()\n",
    "\n",
    "    dataframesInDatenbankSchreiben()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufrufen der Befüllmethode für die Datenbank\n",
    "Weil wir die Dataframes in einer Liste in der selben Reihenfolge wie die Tabellennamen haben, können wir jetzt 5 mal iterieren und copy_from_stringio() mit passenden Parametern aufrufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,5):\n",
    "#     copy_from_stringio(conn, dataframes[i], tablenames[i])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b910aead1e54219b8385bf2103588688280313a03a00fdf13df0f553b4fbbd2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
